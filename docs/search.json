[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "labs/lab03b.html",
    "href": "labs/lab03b.html",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "",
    "text": "In this lab, we will visualize the Multivariate Normal (MVN) distribution, explore how the covariance matrix \\(\\Sigma\\) affects the shape of the density, and verify the Chi-square property for probability contours.\n{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(mvtnorm) # For multivariate normal functions\nlibrary(ellipse) # For drawing contours"
  },
  {
    "objectID": "labs/lab03b.html#introduction",
    "href": "labs/lab03b.html#introduction",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "",
    "text": "In this lab, we will visualize the Multivariate Normal (MVN) distribution, explore how the covariance matrix \\(\\Sigma\\) affects the shape of the density, and verify the Chi-square property for probability contours.\n{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(mvtnorm) # For multivariate normal functions\nlibrary(ellipse) # For drawing contours"
  },
  {
    "objectID": "labs/lab03b.html#defining-the-parameters",
    "href": "labs/lab03b.html#defining-the-parameters",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "1. Defining the Parameters",
    "text": "1. Defining the Parameters\nFrom the lecture, we know a bivariate normal distribution is defined by a mean vector \\(\\boldsymbol{\\mu}\\) and a covariance matrix \\(\\Sigma\\).\n# Define the mean vector mu\nmu &lt;- c(1, 2)\n\n# Define the covariance matrix Sigma (Example: sigma11=1, sigma22=1, rho=0.75)\nrho &lt;- 0.75\nsigma11 &lt;- 1\nsigma22 &lt;- 1\nsigma12 &lt;- rho * sqrt(sigma11) * sqrt(sigma22)\n\nSigma &lt;- matrix(c(sigma11, sigma12, \n                  sigma12, sigma22), nrow = 2)\n\nprint(\"Mean Vector:\")\nprint(mu)\nprint(\"Covariance Matrix:\")\nprint(Sigma)"
  },
  {
    "objectID": "labs/lab03b.html#visualizing-the-bivariate-density",
    "href": "labs/lab03b.html#visualizing-the-bivariate-density",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "2. Visualizing the Bivariate Density",
    "text": "2. Visualizing the Bivariate Density\nWe can visualize the ‚Äúbell shape‚Äù in 3D using the dmvnorm function.\n# Create a grid for X and Y\nx &lt;- seq(-2, 4, length.out = 50)\ny &lt;- seq(-1, 5, length.out = 50)\ngrid &lt;- expand.grid(x = x, y = y)\n\n# Calculate density values\nz &lt;- dmvnorm(as.matrix(grid), mean = mu, sigma = Sigma)\nz_matrix &lt;- matrix(z, nrow = 50)\n\n# Plot 3D surface\npersp(x, y, z_matrix, theta = 30, phi = 30, col = \"lightblue\", \n      shade = 0.75, main = \"3D Bivariate Normal Density\")"
  },
  {
    "objectID": "labs/lab03b.html#constant-probability-contours",
    "href": "labs/lab03b.html#constant-probability-contours",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "3. Constant Probability Contours",
    "text": "3. Constant Probability Contours\nThe lecture states that contours are ellipsoids centered at \\(\\boldsymbol{\\mu}\\). Let‚Äôs plot the \\(50\\%\\) and \\(95\\%\\) probability contours.\nplot(NULL, xlim = c(-2, 4), ylim = c(-1, 5), \n     xlab = \"X1\", ylab = \"X2\", main = \"Probability Contours\")\n\n# Plot 50% contour\nlines(ellipse(Sigma, centre = mu, level = 0.50), col = \"blue\", lwd = 2)\n# Plot 95% contour\nlines(ellipse(Sigma, centre = mu, level = 0.95), col = \"red\", lwd = 2)\n\npoints(mu[1], mu[2], pch = 19, col = \"black\") # Mean center\nlegend(\"topleft\", legend = c(\"50%\", \"95%\"), col = c(\"blue\", \"red\"), lty = 1)"
  },
  {
    "objectID": "labs/lab03b.html#eigenvalues-and-the-shape-of-the-ellipse",
    "href": "labs/lab03b.html#eigenvalues-and-the-shape-of-the-ellipse",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "4. Eigenvalues and the Shape of the Ellipse",
    "text": "4. Eigenvalues and the Shape of the Ellipse\nThe axes of the ellipses are determined by the eigenvectors of \\(\\Sigma\\), and the lengths are proportional to \\(\\sqrt{\\lambda_i}\\).\n# Calculate eigenvalues and eigenvectors\nev &lt;- eigen(Sigma)\n\nprint(\"Eigenvalues (Lengths):\")\nprint(ev$values)\n\nprint(\"Eigenvectors (Directions):\")\nprint(ev$vectors)\nExercise: Notice that the first eigenvector points in the direction of the greatest spread (the major axis of the ellipse)."
  },
  {
    "objectID": "labs/lab03b.html#the-chi-square-property",
    "href": "labs/lab03b.html#the-chi-square-property",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "5. The Chi-Square Property",
    "text": "5. The Chi-Square Property\nThe squared Mahalanobis distance \\((\\mathbf{x}-\\boldsymbol{\\mu})^{\\top} \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\) should follow a \\(\\chi^2\\) distribution with \\(p=2\\) degrees of freedom.\n# Generate 1000 random samples from our MVN distribution\nset.seed(123)\nsamples &lt;- rmvnorm(1000, mean = mu, sigma = Sigma)\n\n# Calculate squared distance for each sample\ndist_sq &lt;- mahalanobis(samples, center = mu, cov = Sigma)\n\n# Compare sample distribution to Chi-square (df=2) using a Q-Q plot\nqqplot(qchisq(ppoints(1000), df = 2), dist_sq,\n       main = \"Chi-Square Q-Q Plot\",\n       xlab = \"Theoretical Chi-square Quantiles\",\n       ylab = \"Sample Squared Distances\")\nabline(0, 1, col = \"red\")"
  },
  {
    "objectID": "labs/lab01b.html",
    "href": "labs/lab01b.html",
    "title": "Lab01b: Matrix Algebra and Image Data",
    "section": "",
    "text": "This computer tasks are short and sweet, as the focus has primarily been on the mathematics. Tasks for later chapters will be more challenging.\n\n\nWe define vectors and matrices using c() and matrix(). Note that by default R fills a matrix by column; we use byrow=TRUE to fill by row.\na &lt;- c(3,1,1,6)\nb &lt;- c(5,6,2,8) \n\nA &lt;- matrix(a, nrow=2, byrow=TRUE) \nB &lt;- matrix(b, nrow=2, byrow=TRUE)\n\n# Matrix Multiplication vs Element-wise Multiplication\nA %*% B # Matrix product \nA * B   # Element-wise product (Hadamard product)\n\n# Transpose and Inner Product\nt(a) %*% b\nInverse, Determinant, and Trace\nsolve(A) # Inverse \ndet(A)   # Determinant \nsum(diag(A)) # Trace (sum of diagonal elements)\n\n# Checking Numerical Error: A * A^-1 should be Identity\nA %*% solve(A)\nSolving Linear SystemsWe can solve for \\(\\mathbf{x}\\) in the system \\(\\mathbf{Ax} = \\mathbf{b}\\) using solve(A, b).\\[\\begin{pmatrix} 3 & 2 & 1 \\\\ 2 & 1 & 3 \\\\ 1 & 3 & 2 \\end{pmatrix} \\mathbf{x} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\]\nA_sys &lt;- matrix(c(3,2,1, 2,1,3, 1,3,2), nrow=3, byrow=TRUE)\nb_sys &lt;- c(1,1,1) \nx_sol &lt;- solve(A_sys, b_sys)\nx_sol\nVectors and Vector Operations\nIn R, a vector is created using the c() function. We can perform scalar multiplication and addition directly.\nExample: Vector Calculations\nGiven \\(\\mathbf{x}^{T}=[1, 3, 2]\\) and \\(\\mathbf{y}^{T}=[-2, 1, -1]\\).\n# Define vectors\nx &lt;- c(1, 3, 2)\ny &lt;- c(-2, 1, -1)\n\n# Scalar Multiplication (3x)\nthree_x &lt;- 3 * x\nprint(three_x)\n\n# Vector Addition (x + y)\nx_plus_y &lt;- x + y\nprint(x_plus_y)\n\n# Length (Norm) function\nvec_norm &lt;- function(v) sqrt(sum(v^2))\n\nLx &lt;- vec_norm(x)\nLy &lt;- vec_norm(y)\nL_3x &lt;- vec_norm(three_x)\n\n# Angle between vectors (theta)\ncos_theta &lt;- sum(x * y) / (Lx * Ly)\ntheta_rad &lt;- acos(cos_theta)\ntheta_deg &lt;- theta_rad * (180 / pi)\n\ncat(\"Length of x:\", Lx, \"\\n\")\ncat(\"Length of y:\", Ly, \"\\n\")\ncat(\"Angle (degrees):\", theta_deg, \"\\n\")\ncat(\"Is Length(3x) == 3 * Length(x)?\", all.equal(L_3x, 3 * Lx), \"\\n\")"
  },
  {
    "objectID": "labs/lab01b.html#computer-task-matrix-computations-in-r",
    "href": "labs/lab01b.html#computer-task-matrix-computations-in-r",
    "title": "Lab01b: Matrix Algebra and Image Data",
    "section": "",
    "text": "This computer tasks are short and sweet, as the focus has primarily been on the mathematics. Tasks for later chapters will be more challenging.\n\n\nWe define vectors and matrices using c() and matrix(). Note that by default R fills a matrix by column; we use byrow=TRUE to fill by row.\na &lt;- c(3,1,1,6)\nb &lt;- c(5,6,2,8) \n\nA &lt;- matrix(a, nrow=2, byrow=TRUE) \nB &lt;- matrix(b, nrow=2, byrow=TRUE)\n\n# Matrix Multiplication vs Element-wise Multiplication\nA %*% B # Matrix product \nA * B   # Element-wise product (Hadamard product)\n\n# Transpose and Inner Product\nt(a) %*% b\nInverse, Determinant, and Trace\nsolve(A) # Inverse \ndet(A)   # Determinant \nsum(diag(A)) # Trace (sum of diagonal elements)\n\n# Checking Numerical Error: A * A^-1 should be Identity\nA %*% solve(A)\nSolving Linear SystemsWe can solve for \\(\\mathbf{x}\\) in the system \\(\\mathbf{Ax} = \\mathbf{b}\\) using solve(A, b).\\[\\begin{pmatrix} 3 & 2 & 1 \\\\ 2 & 1 & 3 \\\\ 1 & 3 & 2 \\end{pmatrix} \\mathbf{x} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\]\nA_sys &lt;- matrix(c(3,2,1, 2,1,3, 1,3,2), nrow=3, byrow=TRUE)\nb_sys &lt;- c(1,1,1) \nx_sol &lt;- solve(A_sys, b_sys)\nx_sol\nVectors and Vector Operations\nIn R, a vector is created using the c() function. We can perform scalar multiplication and addition directly.\nExample: Vector Calculations\nGiven \\(\\mathbf{x}^{T}=[1, 3, 2]\\) and \\(\\mathbf{y}^{T}=[-2, 1, -1]\\).\n# Define vectors\nx &lt;- c(1, 3, 2)\ny &lt;- c(-2, 1, -1)\n\n# Scalar Multiplication (3x)\nthree_x &lt;- 3 * x\nprint(three_x)\n\n# Vector Addition (x + y)\nx_plus_y &lt;- x + y\nprint(x_plus_y)\n\n# Length (Norm) function\nvec_norm &lt;- function(v) sqrt(sum(v^2))\n\nLx &lt;- vec_norm(x)\nLy &lt;- vec_norm(y)\nL_3x &lt;- vec_norm(three_x)\n\n# Angle between vectors (theta)\ncos_theta &lt;- sum(x * y) / (Lx * Ly)\ntheta_rad &lt;- acos(cos_theta)\ntheta_deg &lt;- theta_rad * (180 / pi)\n\ncat(\"Length of x:\", Lx, \"\\n\")\ncat(\"Length of y:\", Ly, \"\\n\")\ncat(\"Angle (degrees):\", theta_deg, \"\\n\")\ncat(\"Is Length(3x) == 3 * Length(x)?\", all.equal(L_3x, 3 * Lx), \"\\n\")"
  },
  {
    "objectID": "labs/lab01b.html#linear-independence-and-projection",
    "href": "labs/lab01b.html#linear-independence-and-projection",
    "title": "Lab01b: Matrix Algebra and Image Data",
    "section": "Linear Independence and Projection",
    "text": "Linear Independence and Projection\n\nExample: Identifying Linearly Independent Vectors\nTo check if vectors are linearly independent, we can bind them into a matrix and check the Rank. If the rank equals the number of vectors, they are linearly independent.\nx1 &lt;- c(1, 2, 1)\nx2 &lt;- c(1, 0, -1)\nx3 &lt;- c(1, -2, 1)\n\nmat_x &lt;- cbind(x1, x2, x3)\nlibrary(Matrix)\nrank &lt;- rankMatrix(mat_x)[1]\n\ncat(\"Rank of the matrix:\", rank, \"\\n\")\n# If rank is 3, they are independent.\n\n\nExample: Matrix Multiplication\nIn R, matrix multiplication uses the %*% operator.\nA &lt;- matrix(c(3, -1, 2, 1, 5, 4), nrow = 2, byrow = TRUE)\nB &lt;- matrix(c(-2, 7, 9), nrow = 3)\n\n# Matrix-Vector product\nAB &lt;- A %*% B\nprint(AB)\n\n# Inner product vs Outer product\nb &lt;- c(7, -3, 6)\ncc &lt;- c(5, 8, -4)\n\ninner &lt;- t(b) %*% cc  # Result is a 1x1 matrix (scalar)\nouter &lt;- b %*% t(cc)  # Result is a 3x3 matrix"
  },
  {
    "objectID": "labs/lab01b.html#eigenvalues-and-eigenvectors",
    "href": "labs/lab01b.html#eigenvalues-and-eigenvectors",
    "title": "Lab01b: Matrix Algebra and Image Data",
    "section": "Eigenvalues and Eigenvectors",
    "text": "Eigenvalues and Eigenvectors\n\nExample: Verifying Eigen-decomposition\nA &lt;- matrix(c(1, -5, -5, 1), nrow = 2, byrow = TRUE)\nev &lt;- eigen(A)\n\ncat(\"Eigenvalues:\\n\")\nprint(ev$values)\n\ncat(\"Eigenvectors (normalized):\\n\")\nprint(ev$vectors)"
  },
  {
    "objectID": "labs/lab01b.html#spectral-decomposition-and-positive-definiteness",
    "href": "labs/lab01b.html#spectral-decomposition-and-positive-definiteness",
    "title": "Lab01b: Matrix Algebra and Image Data",
    "section": "Spectral Decomposition and Positive Definiteness",
    "text": "Spectral Decomposition and Positive Definiteness\nA symmetric matrix is positive definite if all its eigenvalues are \\(&gt; 0\\).\n\nExample 2.9: Spectral Decomposition\nThe spectral decomposition is \\(\\mathbf{A} = \\sum \\lambda_i \\mathbf{e}_i \\mathbf{e}_i^T\\).\nA &lt;- matrix(c(13, -4, 2, \n              -4, 13, -2, \n               2, -2, 10), nrow = 3, byrow = TRUE)\n\ndecomp &lt;- eigen(A)\nL &lt;- decomp$values\nE &lt;- decomp$vectors\n\n# Reconstruct A using spectral decomposition\nA_reconstructed &lt;- L[1]*(E[,1] %*% t(E[,1])) + \n                   L[2]*(E[,2] %*% t(E[,2])) + \n                   L[3]*(E[,3] %*% t(E[,3]))\n\nprint(round(A_reconstructed, 5))\n\n# Check if Positive Definite\nis_pos_def &lt;- all(L &gt; 0)\ncat(\"Is the matrix positive definite?\", is_pos_def)"
  },
  {
    "objectID": "labs/lab01b.html#dataset-application-the-iris-data",
    "href": "labs/lab01b.html#dataset-application-the-iris-data",
    "title": "Lab01b: Matrix Algebra and Image Data",
    "section": "2. Dataset Application: The Iris Data",
    "text": "2. Dataset Application: The Iris Data\nThe Iris dataset contains measurements on 4 numerical variables. We convert it to a matrix to perform multivariate operations.\nX &lt;- as.matrix(iris[, 1:4]) \nn &lt;- nrow(X)\n\n# Sample statistics using built-in commands\nx_bar &lt;- colMeans(X) \nS &lt;- cov(X) \nR &lt;- cor(X)\n\nThe Centering Matrix \\(\\mathbf{H}\\)\nThe centering matrix is defined as \\(\\mathbf{H} = \\mathbf{I}_n - \\frac{1}{n}\\mathbf{1}_n\\mathbf{1}_n^T\\). It is used to shift data so that the mean is zero.\n# Create Centering Matrix \none_n &lt;- rep(1, n) \nH &lt;- diag(n) - (one_n %*% t(one_n)) / n\n\n# Center the data\nX_centered &lt;- H %*% X\n\n# Verify: Column means of HX should be zero\nround(colMeans(X_centered), 10)\nPractical Tip: Computing \\(\\mathbf{H}\\) (a \\(150 \\times 150\\) matrix) is mathematically elegant but computationally expensive for large \\(n\\). In practice, we use the sweep function to subtract means efficiently:\nsweep(X, 2, colMeans(X))"
  },
  {
    "objectID": "labs/lab01b.html#introduction-to-mnist-as-multivariate-data",
    "href": "labs/lab01b.html#introduction-to-mnist-as-multivariate-data",
    "title": "Lab01b: Matrix Algebra and Image Data",
    "section": "3. Introduction to MNIST as Multivariate Data",
    "text": "3. Introduction to MNIST as Multivariate Data\nIn our lectures, we defined a random vector \\(\\mathbf{X}\\) as a collection of random variables. The MNIST dataset consists of grayscale images of handwritten digits. Each image is \\(28 \\times 28\\) pixels, which can be ‚Äúunrolled‚Äù into a vector of length \\(p = 784\\).\n\nLoading the Data\nFirst, ensure the mnist.rda file is in your working directory.\n# Load the dataset\nload('mnist.rda')\n\n# Explore the structure\n# mnist$train contains 60,000 images \n# mnist$test contains 10,000 images\nstr(mnist$train)\n\n\nVisualization Function\nWe define a helper function to reshape the \\(784 \\times 1\\) vectors back into \\(28 \\times 28\\) matrices for plotting.\nplot.mnist &lt;- function(im){\n  if(is.vector(im)){ \n    # A single image \n    A &lt;- matrix(im, nr=28, byrow=F) \n    C &lt;- melt(A, varnames = c(\"x\", \"y\"), value.name = \"intensity\") \n    p &lt;- ggplot(C, aes(x = x, y = y, fill = intensity)) + \n      geom_tile() + \n      scale_fill_gradient(low='white', high='black') + \n      scale_y_reverse() + \n      theme_void() + \n      theme(legend.position = \"none\") \n  } else { \n    # Multiple images \n    if (dim(im)[2] != 784) { im = t(im) } \n    n &lt;- dim(im)[1] \n    As &lt;- array(im, dim = c(n, 28, 28)) \n    Cs &lt;- melt(As, varnames = c(\"image\",\"x\", \"y\"), value.name = \"intensity\") \n    p &lt;- ggplot(Cs, aes(x = x, y = y, fill = intensity)) + \n      geom_tile() + \n      scale_fill_gradient(low='white', high='black') + \n      facet_wrap(~ image, nrow = floor(sqrt(n))+1) + \n      scale_y_reverse() + \n      theme_void() + \n      theme(legend.position = \"none\", panel.spacing = unit(0, \"lines\")) \n  } \n  return(p) \n}\n\n\nTask: Plotting the first 10 images\nLet‚Äôs look at the first 10 rows of the training matrix. Each row is a unique observation (image).\n# Select the first 10 images\nfirst_10 &lt;- mnist$train$x[1:10, ]\n\n# Plot them\nplot.mnist(first_10)\n\n\nTask: Filtering and Plotting a specific digit (The ‚Äô5‚Äôs)\nIn multivariate analysis, we often want to look at subsets of our data. Here, we filter the dataset based on the labels provided in mnist$train$y.\n# Identify indices where the label is 5 \nindex_5 &lt;- which(mnist$train$y == 5)\n\n# Select the first 16 occurrences of the digit 5\nfives_subset &lt;- mnist$train$x[index_5[1:16], ]\n\n# Plot the selection\nplot.mnist(fives_subset)"
  },
  {
    "objectID": "labs/lab01b.html#lab-questions",
    "href": "labs/lab01b.html#lab-questions",
    "title": "Lab01b: Matrix Algebra and Image Data",
    "section": "Lab Questions:",
    "text": "Lab Questions:\n\nDimensionality: Each image is a vector \\(\\mathbf{X}\\). What is the dimensionality \\(p\\) of this vector?\nMean Vector: Calculate the ‚ÄúAverage 5.‚Äù Compute the column means of all images that are labeled as ‚Äò5‚Äô and plot the result using plot.mnist(). Does the result look like a typical ‚Äò5‚Äô?\n\nHint: colMeans(mnist$train$x[index_5, ])\n\nVariance: Based on your lecture notes on the Covariance Matrix \\(\\mathbf{S}\\), which pixels do you expect to have the highest variance in a dataset of handwritten ‚Äô5‚Äôs? (The center pixels or the edge pixels?)"
  },
  {
    "objectID": "labs/lab02.html",
    "href": "labs/lab02.html",
    "title": "Lab02a: Random Vectors and Matrices",
    "section": "",
    "text": "The Euclidean distance between a point \\(P = (x_1, x_2)\\) and the origin \\(O = (0,0)\\) is \\(d(O, P) = \\sqrt{x_1^2 + x_2^2}\\). However, in statistics, we must account for different variances.\n\n\nSuppose we have a point \\(P = (3, 3)\\).\n\nCalculate the Euclidean distance.\nCalculate the Statistical distance assuming \\(s_{11} = 9\\) and \\(s_{22} = 1\\).\n\n# Point P\nx &lt;- c(3, 3)\n\n# 1. Euclidean Distance\nd_euc &lt;- sqrt(sum(x^2))\nprint(paste(\"Euclidean Distance:\", round(d_euc, 4)))\n\n# 2. Statistical Distance (Standardized)\ns11 &lt;- 9\ns22 &lt;- 1\nd_stat &lt;- sqrt(x[1]^2/s11 + x[2]^2/s22)\nprint(paste(\"Statistical Distance:\", round(d_stat, 4)))\nObservation: Notice how the statistical distance penalizes the dimension with lower variability (\\(x_2\\)) more heavily."
  },
  {
    "objectID": "labs/lab02.html#euclidean-vs.-statistical-distance",
    "href": "labs/lab02.html#euclidean-vs.-statistical-distance",
    "title": "Lab02a: Random Vectors and Matrices",
    "section": "",
    "text": "The Euclidean distance between a point \\(P = (x_1, x_2)\\) and the origin \\(O = (0,0)\\) is \\(d(O, P) = \\sqrt{x_1^2 + x_2^2}\\). However, in statistics, we must account for different variances.\n\n\nSuppose we have a point \\(P = (3, 3)\\).\n\nCalculate the Euclidean distance.\nCalculate the Statistical distance assuming \\(s_{11} = 9\\) and \\(s_{22} = 1\\).\n\n# Point P\nx &lt;- c(3, 3)\n\n# 1. Euclidean Distance\nd_euc &lt;- sqrt(sum(x^2))\nprint(paste(\"Euclidean Distance:\", round(d_euc, 4)))\n\n# 2. Statistical Distance (Standardized)\ns11 &lt;- 9\ns22 &lt;- 1\nd_stat &lt;- sqrt(x[1]^2/s11 + x[2]^2/s22)\nprint(paste(\"Statistical Distance:\", round(d_stat, 4)))\nObservation: Notice how the statistical distance penalizes the dimension with lower variability (\\(x_2\\)) more heavily."
  },
  {
    "objectID": "labs/lab02.html#the-square-root-matrix",
    "href": "labs/lab02.html#the-square-root-matrix",
    "title": "Lab02a: Random Vectors and Matrices",
    "section": "2. The Square-Root Matrix",
    "text": "2. The Square-Root Matrix\nFor a positive definite matrix \\(\\mathbf{A}\\), the square-root matrix \\(\\mathbf{A}^{1/2}\\) is defined via spectral decomposition: \\(\\mathbf{A}^{1/2} = \\mathbf{P} \\Lambda^{1/2} \\mathbf{P}^T\\).\n\nExercise: Compute \\(A^{1/2}\\)\nGiven \\(\\mathbf{A} = \\begin{bmatrix} 4 & 1 \\\\ 1 & 9 \\end{bmatrix}\\):\nA &lt;- matrix(c(4, 1, 1, 9), nrow = 2)\n\n# Spectral Decomposition\ndecomp &lt;- eigen(A)\nP &lt;- decomp$vectors\nLambda &lt;- diag(decomp$values)\n\n# Compute Square Root Matrix\nA_sqrt &lt;- P %*% diag(sqrt(decomp$values)) %*% t(P)\n\nprint(\"Square Root Matrix A^(1/2):\")\nprint(A_sqrt)\n\n# Verification: A_sqrt * A_sqrt should equal A\nprint(\"Verification (A_sqrt %*% A_sqrt):\")\nprint(round(A_sqrt %*% A_sqrt, 10))"
  },
  {
    "objectID": "labs/lab02.html#random-vectors-and-covariance",
    "href": "labs/lab02.html#random-vectors-and-covariance",
    "title": "Lab02a: Random Vectors and Matrices",
    "section": "3. Random Vectors and Covariance",
    "text": "3. Random Vectors and Covariance\nThe population correlation matrix \\(\\rho\\) is derived from the covariance matrix \\(\\Sigma\\) by \\(\\rho_{ik} = \\frac{\\sigma_{ik}}{\\sqrt{\\sigma_{ii}}\\sqrt{\\sigma_{kk}}}\\).\n\nExercise: Example 2.13\nGiven \\(\\Sigma = \\begin{bmatrix} 4 & 1 & 2 \\\\ 1 & 9 & -3 \\\\ 2 & -3 & 25 \\end{bmatrix}\\), find the correlation matrix.\nSigma &lt;- matrix(c(4, 1, 2, \n                  1, 9, -3, \n                  2, -3, 25), nrow = 3, byrow = TRUE)\n\n# Method 1: Manual Calculation using diagonals\nD_inv_sqrt &lt;- diag(1/sqrt(diag(Sigma)))\nrho_manual &lt;- D_inv_sqrt %*% Sigma %*% D_inv_sqrt\n\n# Method 2: Using R's cov2cor function\nrho_r &lt;- cov2cor(Sigma)\n\nprint(\"Correlation Matrix:\")\nprint(rho_r)"
  },
  {
    "objectID": "labs/lab02.html#maximization-lemma",
    "href": "labs/lab02.html#maximization-lemma",
    "title": "Lab02a: Random Vectors and Matrices",
    "section": "4. Maximization Lemma",
    "text": "4. Maximization Lemma\nThe lecture notes state that for a positive definite matrix \\(\\mathbf{B}\\), the maximum value of \\(\\frac{(\\mathbf{x}^T \\mathbf{d})^2}{\\mathbf{x}^T \\mathbf{B} \\mathbf{x}}\\) is \\(\\mathbf{d}^T \\mathbf{B}^{-1} \\mathbf{d}\\).\n\nExercise: Verify Maximization\nLet \\(\\mathbf{B} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 4 \\end{bmatrix}\\) and \\(\\mathbf{d} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\).\nB &lt;- matrix(c(2, 1, 1, 4), nrow = 2)\nd &lt;- c(1, 1)\n\n# Theoretical Maximum\ntheo_max &lt;- t(d) %*% solve(B) %*% d\n\n# Attained at x = c * B^-1 * d\nx_opt &lt;- solve(B) %*% d\n\n# Verify\nempirical_max &lt;- (t(x_opt) %*% d)^2 / (t(x_opt) %*% B %*% x_opt)\n\nprint(paste(\"Theoretical Max:\", theo_max))\nprint(paste(\"Value at optimal x:\", empirical_max))"
  },
  {
    "objectID": "labs/lab02.html#visualizing-constant-distance-ellipses",
    "href": "labs/lab02.html#visualizing-constant-distance-ellipses",
    "title": "Lab02a: Random Vectors and Matrices",
    "section": "5. Visualizing Constant Distance (Ellipses)",
    "text": "5. Visualizing Constant Distance (Ellipses)\nPoints of constant statistical distance \\(\\mathbf{x}^T \\mathbf{A} \\mathbf{x} = c^2\\) form an ellipse.\n# Define a covariance matrix\nSigma_plot &lt;- matrix(c(5, 2, 2, 2), nrow = 2)\nA_mat &lt;- solve(Sigma_plot) # Inverse for distance calculation\n\n# Generate points for an ellipse\ntheta &lt;- seq(0, 2*pi, length.out = 100)\nz &lt;- cbind(cos(theta), sin(theta))\n\n# Decompose A to transform the circle into an ellipse\ned &lt;- eigen(Sigma_plot)\n# Points = Eigenvectors * sqrt(Eigenvalues) * unit_circle\nellipse_pts &lt;- z %*% diag(sqrt(ed$values)) %*% t(ed$vectors)\nellipse_df &lt;- as.data.frame(ellipse_pts)\n\nggplot(ellipse_df, aes(V1, V2)) +\n  geom_path(color = \"blue\") +\n  coord_fixed() +\n  labs(title = \"Constant Statistical Distance Ellipse\",\n       x = \"x1\", y = \"x2\") +\n  theme_minimal()"
  },
  {
    "objectID": "labs/lab01a.html",
    "href": "labs/lab01a.html",
    "title": "Lab 01",
    "section": "",
    "text": "If you haven‚Äôt done so already, please download and install R and RStudio. R is the programming language, and RStudio is an integrated development environment that makes using R much more pleasurable. My advice is to always use RStudio and never run code in R itself.\nFor complete beginners: For those who are completely new to R (or those who want a refresher), I recommend working through an online tutorial."
  },
  {
    "objectID": "labs/lab01a.html#getting-started",
    "href": "labs/lab01a.html#getting-started",
    "title": "Lab 01",
    "section": "",
    "text": "If you haven‚Äôt done so already, please download and install R and RStudio. R is the programming language, and RStudio is an integrated development environment that makes using R much more pleasurable. My advice is to always use RStudio and never run code in R itself.\nFor complete beginners: For those who are completely new to R (or those who want a refresher), I recommend working through an online tutorial."
  },
  {
    "objectID": "labs/lab01a.html#warm-up-the-iris-dataset",
    "href": "labs/lab01a.html#warm-up-the-iris-dataset",
    "title": "Lab 01",
    "section": "Warm-up: The Iris Dataset",
    "text": "Warm-up: The Iris Dataset\nThe most important aspects of R to focus on for this module are Basic plotting and Manipulation of matrices and data frames. Let‚Äôs look at the built-in iris dataset.\n\nExercise 1\nCan you plot the sepal length against the sepal width?\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nData Manipulation\nTasks: Select the column of the iris data that contains sepal length.\n\nSelect only the rows for species setosa.\nThere are several ways to do basic data manipulation in R. You can use base R commands or the dplyr commands (part of tidyverse).\n\n library(dplyr)\n# Selecting and Filtering\niris |&gt; \n  filter(Species == 'setosa') |&gt; \n  summarise(mean_petal = mean(Petal.Length))"
  },
  {
    "objectID": "labs/lab01a.html#matrix-operations",
    "href": "labs/lab01a.html#matrix-operations",
    "title": "Lab 01",
    "section": "Matrix Operations",
    "text": "Matrix Operations\nLet‚Äôs extract the four numerical columns and store them as a matrix \\(X\\).\nX &lt;- as.matrix(iris[,1:4])\nD &lt;- diag(1:4) # Diagonal matrix\nX_weighted &lt;- X %*% D\nhead(X_weighted)"
  },
  {
    "objectID": "labs/lab01a.html#sample-statistics",
    "href": "labs/lab01a.html#sample-statistics",
    "title": "Lab 01",
    "section": "Sample Statistics",
    "text": "Sample Statistics\nCalculate the sample mean, covariance, and correlation for the following student data.\nEx1 &lt;- data.frame(\n  Student = LETTERS[1:5],\n  P = c(41, 72, 46, 77, 59),\n  S = c(63, 82, 38, 57, 85)\n)\n\ncolMeans(Ex1[, 2:3])\ncov(Ex1[, 2:3])"
  },
  {
    "objectID": "labs/lab01a.html#multivariate-normal-distribution",
    "href": "labs/lab01a.html#multivariate-normal-distribution",
    "title": "Lab 01",
    "section": "Multivariate Normal Distribution",
    "text": "Multivariate Normal Distribution\nGenerate 100 samples from the multivariate normal distribution with \\(\\mu = (1, 0)^T\\) and \\(\\Sigma = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\\).\nlibrary(mvtnorm)\nmu &lt;- c(1, 0)\nSigma &lt;- matrix(c(2, 1, 1, 2), nr=2)\nset.seed(843)\nX_mvn &lt;- rmvnorm(n=100, mean=mu, sigma=Sigma)\nplot(X_mvn, main=\"Synthetic MVN Samples\")"
  },
  {
    "objectID": "labs/lab03a.html",
    "href": "labs/lab03a.html",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "",
    "text": "In this lab, we will visualize the Multivariate Normal (MVN) distribution, explore how the covariance matrix \\(\\Sigma\\) affects the shape of the density, and verify the Chi-square property for probability contours.\n{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(mvtnorm) # For multivariate normal functions\nlibrary(ellipse) # For drawing contours"
  },
  {
    "objectID": "labs/lab03a.html#introduction",
    "href": "labs/lab03a.html#introduction",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "",
    "text": "In this lab, we will visualize the Multivariate Normal (MVN) distribution, explore how the covariance matrix \\(\\Sigma\\) affects the shape of the density, and verify the Chi-square property for probability contours.\n{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(mvtnorm) # For multivariate normal functions\nlibrary(ellipse) # For drawing contours"
  },
  {
    "objectID": "labs/lab03a.html#defining-the-parameters",
    "href": "labs/lab03a.html#defining-the-parameters",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "1. Defining the Parameters",
    "text": "1. Defining the Parameters\nFrom the lecture, we know a bivariate normal distribution is defined by a mean vector \\(\\boldsymbol{\\mu}\\) and a covariance matrix \\(\\Sigma\\).\n# Define the mean vector mu\nmu &lt;- c(1, 2)\n\n# Define the covariance matrix Sigma (Example: sigma11=1, sigma22=1, rho=0.75)\nrho &lt;- 0.75\nsigma11 &lt;- 1\nsigma22 &lt;- 1\nsigma12 &lt;- rho * sqrt(sigma11) * sqrt(sigma22)\n\nSigma &lt;- matrix(c(sigma11, sigma12, \n                  sigma12, sigma22), nrow = 2)\n\nprint(\"Mean Vector:\")\nprint(mu)\nprint(\"Covariance Matrix:\")\nprint(Sigma)"
  },
  {
    "objectID": "labs/lab03a.html#visualizing-the-bivariate-density",
    "href": "labs/lab03a.html#visualizing-the-bivariate-density",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "2. Visualizing the Bivariate Density",
    "text": "2. Visualizing the Bivariate Density\nWe can visualize the ‚Äúbell shape‚Äù in 3D using the dmvnorm function.\n# Create a grid for X and Y\nx &lt;- seq(-2, 4, length.out = 50)\ny &lt;- seq(-1, 5, length.out = 50)\ngrid &lt;- expand.grid(x = x, y = y)\n\n# Calculate density values\nz &lt;- dmvnorm(as.matrix(grid), mean = mu, sigma = Sigma)\nz_matrix &lt;- matrix(z, nrow = 50)\n\n# Plot 3D surface\npersp(x, y, z_matrix, theta = 30, phi = 30, col = \"lightblue\", \n      shade = 0.75, main = \"3D Bivariate Normal Density\")"
  },
  {
    "objectID": "labs/lab03a.html#constant-probability-contours",
    "href": "labs/lab03a.html#constant-probability-contours",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "3. Constant Probability Contours",
    "text": "3. Constant Probability Contours\nThe lecture states that contours are ellipsoids centered at \\(\\boldsymbol{\\mu}\\). Let‚Äôs plot the \\(50\\%\\) and \\(95\\%\\) probability contours.\nplot(NULL, xlim = c(-2, 4), ylim = c(-1, 5), \n     xlab = \"X1\", ylab = \"X2\", main = \"Probability Contours\")\n\n# Plot 50% contour\nlines(ellipse(Sigma, centre = mu, level = 0.50), col = \"blue\", lwd = 2)\n# Plot 95% contour\nlines(ellipse(Sigma, centre = mu, level = 0.95), col = \"red\", lwd = 2)\n\npoints(mu[1], mu[2], pch = 19, col = \"black\") # Mean center\nlegend(\"topleft\", legend = c(\"50%\", \"95%\"), col = c(\"blue\", \"red\"), lty = 1)"
  },
  {
    "objectID": "labs/lab03a.html#eigenvalues-and-the-shape-of-the-ellipse",
    "href": "labs/lab03a.html#eigenvalues-and-the-shape-of-the-ellipse",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "4. Eigenvalues and the Shape of the Ellipse",
    "text": "4. Eigenvalues and the Shape of the Ellipse\nThe axes of the ellipses are determined by the eigenvectors of \\(\\Sigma\\), and the lengths are proportional to \\(\\sqrt{\\lambda_i}\\).\n# Calculate eigenvalues and eigenvectors\nev &lt;- eigen(Sigma)\n\nprint(\"Eigenvalues (Lengths):\")\nprint(ev$values)\n\nprint(\"Eigenvectors (Directions):\")\nprint(ev$vectors)\nExercise: Notice that the first eigenvector points in the direction of the greatest spread (the major axis of the ellipse)."
  },
  {
    "objectID": "labs/lab03a.html#the-chi-square-property",
    "href": "labs/lab03a.html#the-chi-square-property",
    "title": "Lab03a: The Multivariate Normal Distribution",
    "section": "5. The Chi-Square Property",
    "text": "5. The Chi-Square Property\nThe squared Mahalanobis distance \\((\\mathbf{x}-\\boldsymbol{\\mu})^{\\top} \\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\) should follow a \\(\\chi^2\\) distribution with \\(p=2\\) degrees of freedom.\n# Generate 1000 random samples from our MVN distribution\nset.seed(123)\nsamples &lt;- rmvnorm(1000, mean = mu, sigma = Sigma)\n\n# Calculate squared distance for each sample\ndist_sq &lt;- mahalanobis(samples, center = mu, cov = Sigma)\n\n# Compare sample distribution to Chi-square (df=2) using a Q-Q plot\nqqplot(qchisq(ppoints(1000), df = 2), dist_sq,\n       main = \"Chi-Square Q-Q Plot\",\n       xlab = \"Theoretical Chi-square Quantiles\",\n       ylab = \"Sample Squared Distances\")\nabline(0, 1, col = \"red\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STT843: Multivariate Data Analysis",
    "section": "",
    "text": "Welcome to the course website for STT843 This course provides an introduction to the statistical analysis of multivariate data, focusing on both theoretical foundations and practical applications in R."
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "STT843: Multivariate Data Analysis",
    "section": "üìã Course Information",
    "text": "üìã Course Information\n\nüéì Instructor: Dr.¬†Guanqun Cao\nüéì TA: TBA\n‚è∞ Office Hours: Monday, 10am ‚Äì 11am | C426 Wells Hall\nüìÑ Syllabus: click here üîó"
  },
  {
    "objectID": "index.html#weekly-schedule",
    "href": "index.html#weekly-schedule",
    "title": "STT843: Multivariate Data Analysis",
    "section": "üóìÔ∏è Weekly Schedule",
    "text": "üóìÔ∏è Weekly Schedule\n\n\n\n\n\n\nImportant Note\n\n\n\nLecture slides are updated every Sunday evening. Please ensure you have the latest version before Monday‚Äôs class.\n\n\n\n\n\nWeek\nTopic\nSlides (PDF)\nLabs (Rmd/Qmd)\nAssignment\n\n\n\n\n1\nIntroduction\nLec 1.1 üîóLec 1.2 üîó\nüíª Lab 1aüíª Lab 1b\n‚Äì\n\n\n2\nRandom Vectors and Matrices\nLec 2.1 üîó Lec 2.1* üîó\nüíª Lab 2a\nHW 1\n\n\n3\nMultivariate Normal Distribution\nLec 3.1 üîóLec 3.2 üîó\nüíª Lab 3aüíª Lab 3b\nQuiz 1"
  },
  {
    "objectID": "index.html#software-requirements",
    "href": "index.html#software-requirements",
    "title": "STT843: Multivariate Data Analysis",
    "section": "üõ†Ô∏è Software Requirements",
    "text": "üõ†Ô∏è Software Requirements\nWe will be using R and RStudio for all computations. Please ensure you have the following installed:\n1. [R (version 4.3+)](https://cran.r-project.org/)\n2. [RStudio Desktop](https://posit.co/download/rstudio-desktop/)\n3. [Quarto CLI](https://quarto.org/docs/get-started/)"
  }
]