---
title: "Lab01b: Matrix Algebra and Image Data"
subtitle: "STT 843: Multivariate Analysis"
author: "Dr. Guanqun Cao"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE)
library(reshape2)
library(ggplot2)
```

## 1. Computer Task: Matrix Computations in R

This computer tasks are short and sweet, as the focus has primarily been
on the mathematics. Tasks for later chapters will be more challenging.

### Basic Operations

We define vectors and matrices using `c()` and `matrix()`. Note that by
default R fills a matrix by column; we use `byrow=TRUE` to fill by row.

``` r
a <- c(3,1,1,6)
b <- c(5,6,2,8) 

A <- matrix(a, nrow=2, byrow=TRUE) 
B <- matrix(b, nrow=2, byrow=TRUE)

# Matrix Multiplication vs Element-wise Multiplication
A %*% B # Matrix product 
A * B   # Element-wise product (Hadamard product)

# Transpose and Inner Product
t(a) %*% b
```

**Inverse, Determinant, and Trace**

``` r
solve(A) # Inverse 
det(A)   # Determinant 
sum(diag(A)) # Trace (sum of diagonal elements)

# Checking Numerical Error: A * A^-1 should be Identity
A %*% solve(A)
```

Solving Linear SystemsWe can solve for $\mathbf{x}$ in the system
$\mathbf{Ax} = \mathbf{b}$ using solve(A,
b).$$\begin{pmatrix} 3 & 2 & 1 \\ 2 & 1 & 3 \\ 1 & 3 & 2 \end{pmatrix} \mathbf{x} = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$$

``` r
A_sys <- matrix(c(3,2,1, 2,1,3, 1,3,2), nrow=3, byrow=TRUE)
b_sys <- c(1,1,1) 
x_sol <- solve(A_sys, b_sys)
x_sol
```

**Vectors and Vector Operations**

In R, a vector is created using the c() function. We can perform scalar
multiplication and addition directly.

**Example: Vector Calculations**

Given $\mathbf{x}^{T}=[1, 3, 2]$ and $\mathbf{y}^{T}=[-2, 1, -1]$.

``` R
# Define vectors
x <- c(1, 3, 2)
y <- c(-2, 1, -1)

# Scalar Multiplication (3x)
three_x <- 3 * x
print(three_x)

# Vector Addition (x + y)
x_plus_y <- x + y
print(x_plus_y)

# Length (Norm) function
vec_norm <- function(v) sqrt(sum(v^2))

Lx <- vec_norm(x)
Ly <- vec_norm(y)
L_3x <- vec_norm(three_x)

# Angle between vectors (theta)
cos_theta <- sum(x * y) / (Lx * Ly)
theta_rad <- acos(cos_theta)
theta_deg <- theta_rad * (180 / pi)

cat("Length of x:", Lx, "\n")
cat("Length of y:", Ly, "\n")
cat("Angle (degrees):", theta_deg, "\n")
cat("Is Length(3x) == 3 * Length(x)?", all.equal(L_3x, 3 * Lx), "\n")
```

## Linear Independence and Projection

### Example: Identifying Linearly Independent Vectors

To check if vectors are linearly independent, we can bind them into a
matrix and check the **Rank**. If the rank equals the number of vectors,
they are linearly independent.

``` R
x1 <- c(1, 2, 1)
x2 <- c(1, 0, -1)
x3 <- c(1, -2, 1)

mat_x <- cbind(x1, x2, x3)
library(Matrix)
rank <- rankMatrix(mat_x)[1]

cat("Rank of the matrix:", rank, "\n")
# If rank is 3, they are independent.
```

### Example: Matrix Multiplication

In R, matrix multiplication uses the `%*%` operator.

``` R
A <- matrix(c(3, -1, 2, 1, 5, 4), nrow = 2, byrow = TRUE)
B <- matrix(c(-2, 7, 9), nrow = 3)

# Matrix-Vector product
AB <- A %*% B
print(AB)

# Inner product vs Outer product
b <- c(7, -3, 6)
cc <- c(5, 8, -4)

inner <- t(b) %*% cc  # Result is a 1x1 matrix (scalar)
outer <- b %*% t(cc)  # Result is a 3x3 matrix
```

## Eigenvalues and Eigenvectors

### Example: Verifying Eigen-decomposition

``` R
A <- matrix(c(1, -5, -5, 1), nrow = 2, byrow = TRUE)
ev <- eigen(A)

cat("Eigenvalues:\n")
print(ev$values)

cat("Eigenvectors (normalized):\n")
print(ev$vectors)
```

## Spectral Decomposition and Positive Definiteness

A symmetric matrix is positive definite if all its eigenvalues are
$> 0$.

### Example 2.9: Spectral Decomposition

The spectral decomposition is
$\mathbf{A} = \sum \lambda_i \mathbf{e}_i \mathbf{e}_i^T$.

``` R
A <- matrix(c(13, -4, 2, 
              -4, 13, -2, 
               2, -2, 10), nrow = 3, byrow = TRUE)

decomp <- eigen(A)
L <- decomp$values
E <- decomp$vectors

# Reconstruct A using spectral decomposition
A_reconstructed <- L[1]*(E[,1] %*% t(E[,1])) + 
                   L[2]*(E[,2] %*% t(E[,2])) + 
                   L[3]*(E[,3] %*% t(E[,3]))

print(round(A_reconstructed, 5))

# Check if Positive Definite
is_pos_def <- all(L > 0)
cat("Is the matrix positive definite?", is_pos_def)
```

## 2. Dataset Application: The Iris Data

The Iris dataset contains measurements on 4 numerical variables. We
convert it to a matrix to perform multivariate operations.

``` r
X <- as.matrix(iris[, 1:4]) 
n <- nrow(X)

# Sample statistics using built-in commands
x_bar <- colMeans(X) 
S <- cov(X) 
R <- cor(X)
```

### The Centering Matrix $\mathbf{H}$

The centering matrix is defined as
$\mathbf{H} = \mathbf{I}_n - \frac{1}{n}\mathbf{1}_n\mathbf{1}_n^T$. It
is used to shift data so that the mean is zero.

``` r
# Create Centering Matrix 
one_n <- rep(1, n) 
H <- diag(n) - (one_n %*% t(one_n)) / n

# Center the data
X_centered <- H %*% X

# Verify: Column means of HX should be zero
round(colMeans(X_centered), 10)
```

**Practical Tip:** Computing $\mathbf{H}$ (a $150 \times 150$ matrix) is
mathematically elegant but computationally expensive for large $n$. In
practice, we use the `sweep` function to subtract means efficiently:

``` r
sweep(X, 2, colMeans(X))
```

## 3. Introduction to MNIST as Multivariate Data

In our lectures, we defined a random vector $\mathbf{X}$ as a collection
of random variables. The MNIST dataset consists of grayscale images of
handwritten digits. Each image is $28 \times 28$ pixels, which can be
"unrolled" into a vector of length $p = 784$.

### Loading the Data

First, ensure the `mnist.rda` file is in your working directory.

``` r
# Load the dataset
load('mnist.rda')

# Explore the structure
# mnist$train contains 60,000 images 
# mnist$test contains 10,000 images
str(mnist$train)
```

### Visualization Function

We define a helper function to reshape the $784 \times 1$ vectors back
into $28 \times 28$ matrices for plotting.

``` r
plot.mnist <- function(im){
  if(is.vector(im)){ 
    # A single image 
    A <- matrix(im, nr=28, byrow=F) 
    C <- melt(A, varnames = c("x", "y"), value.name = "intensity") 
    p <- ggplot(C, aes(x = x, y = y, fill = intensity)) + 
      geom_tile() + 
      scale_fill_gradient(low='white', high='black') + 
      scale_y_reverse() + 
      theme_void() + 
      theme(legend.position = "none") 
  } else { 
    # Multiple images 
    if (dim(im)[2] != 784) { im = t(im) } 
    n <- dim(im)[1] 
    As <- array(im, dim = c(n, 28, 28)) 
    Cs <- melt(As, varnames = c("image","x", "y"), value.name = "intensity") 
    p <- ggplot(Cs, aes(x = x, y = y, fill = intensity)) + 
      geom_tile() + 
      scale_fill_gradient(low='white', high='black') + 
      facet_wrap(~ image, nrow = floor(sqrt(n))+1) + 
      scale_y_reverse() + 
      theme_void() + 
      theme(legend.position = "none", panel.spacing = unit(0, "lines")) 
  } 
  return(p) 
}
```

### Task: Plotting the first 10 images

Let's look at the first 10 rows of the training matrix. Each row is a
unique observation (image).

``` r
# Select the first 10 images
first_10 <- mnist$train$x[1:10, ]

# Plot them
plot.mnist(first_10)
```

### Task: Filtering and Plotting a specific digit (The '5's)

In multivariate analysis, we often want to look at subsets of our data.
Here, we filter the dataset based on the labels provided in
`mnist$train$y`.

``` r
# Identify indices where the label is 5 
index_5 <- which(mnist$train$y == 5)

# Select the first 16 occurrences of the digit 5
fives_subset <- mnist$train$x[index_5[1:16], ]

# Plot the selection
plot.mnist(fives_subset)
```

## Lab Questions:

1.  **Dimensionality:** Each image is a vector $\mathbf{X}$. What is the
    dimensionality $p$ of this vector?

2.  **Mean Vector:** Calculate the "Average 5." Compute the column means
    of all images that are labeled as '5' and plot the result using
    `plot.mnist()`. Does the result look like a typical '5'?

    -   *Hint:* `colMeans(mnist$train$x[index_5, ])`

3.  **Variance:** Based on your lecture notes on the Covariance Matrix
    $\mathbf{S}$, which pixels do you expect to have the highest
    variance in a dataset of handwritten '5's? (The center pixels or the
    edge pixels?)
