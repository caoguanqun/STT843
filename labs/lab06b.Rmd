---
title: "Lab 06b: Inference and Subset Selection in MMR"
subtitle: "Unbiased Estimation, Hypothesis Testing, and Selection"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(car)    # For Anova function
library(MASS)   # For dataset examples
```

## 1. Estimating the Error Covariance Matrix ($\Sigma$)

The lecture notes show that an unbiased estimator for the error covariance $\Sigma$ is:

\$\$S = \\frac{\\mathbf{E}}{n-q-1} = \\frac{\\hat{\\Xi}\^T \\hat{\\Xi}}{n-q-1}\$\$

Using the variables `E_hat` and `n`, `q` from Part 1:

``` r
# Assuming n = 100, q = 2 (number of predictors excluding intercept)
q <- 2
n <- 100
E_matrix <- t(E_hat) %*% E_hat
S_unbiased <- E_matrix / (n - q - 1)

print("Unbiased Estimate of Sigma (S):")
print(S_unbiased)

# Comparison with MLE of Sigma (which divides by n)
Sigma_MLE <- E_matrix / n
```

## 2. Variance of the Coefficients

The variance-covariance matrix of the stacked coefficients $\hat{\beta}$ is $\Sigma \otimes (X^T X)^{-1}$.

``` r
# (X'X)^-1
XtX_inv <- solve(t(X) %*% X)

# Variance-Covariance matrix of vec(B_hat)
Var_Beta_hat <- kronecker(S_unbiased, XtX_inv)

# Standard errors of coefficients are the square roots of the diagonal
SE_beta <- sqrt(diag(Var_Beta_hat))
print("Standard Errors for all Beta coefficients:")
print(SE_beta)
```

## 3. Multivariate Hypothesis Testing

We test the overall regression $H_0: \mathbf{B}_1 = 0$ (all predictors except intercept are zero).

### Manual Calculation of Wilks' Lambda

We calculate the **H** (Hypothesis) and **E** (Error) matrices.

``` r
# Mean-corrected Y
Y_bar_mat <- matrix(colMeans(Y), n, ncol(Y), byrow = TRUE)
Total_Corrected <- t(Y) %*% Y - n * (colMeans(Y) %3*% t(colMeans(Y)))

# H matrix (Effect matrix)
H_matrix <- Model_SSCP - n * (colMeans(Y) %*% t(colMeans(Y)))

# Wilks' Lambda calculation
wilks_lambda <- det(E_matrix) / det(E_matrix + H_matrix)
print(paste("Wilks' Lambda:", round(wilks_lambda, 4)))
```

### Using R's `Anova` Function

The `car` package provides an easy way to extract Wilks', Pillai's, and Roy's statistics.

``` r
# Refit using lm for the Anova function
fit_multi <- lm(Y ~ X[, 2] + X[, 3])
manova_results <- Anova(fit_multi, test.statistic = "Wilks")
print(manova_results)
```

## 4. Full vs. Reduced Model (Partial $\Lambda$)

To test if a subset of predictors $h$ is significant given the others are in the model:

\$\$\\Lambda\_{partial} = \\frac{\\Lambda\_{full}}{\\Lambda\_{red}}\$\$

``` r
# Reduced model (only intercept and first predictor)
fit_red <- lm(Y ~ X[, 2])
lambda_full <- summary(manova(fit_multi), test = "Wilks")$stats[1, 3]
lambda_red <- summary(manova(fit_red), test = "Wilks")$stats[1, 3]

lambda_partial <- lambda_full / lambda_red
print(paste("Partial Wilks' Lambda for X3 | X2:", round(lambda_partial, 4)))
```

## 5. Forward Selection Logic (Step 1)

This loop demonstrates the logic of the first step of Forward Selection by calculating $\Lambda$ for each candidate variable individually.

``` r
# Candidate predictors (assuming we have a pool in a dataframe df_X)
candidates <- 2:ncol(X)
results <- data.frame(Variable = candidates, Lambda = NA)

for(i in seq_along(candidates)) {
  temp_fit <- lm(Y ~ X[, candidates[i]])
  results$Lambda[i] <- summary(manova(temp_fit), test = "Wilks")$stats[1, 3]
}

# The variable with the minimum Lambda is the best candidate to add
best_var <- results$Variable[which.min(results$Lambda)]
print(paste("Variable to add first:", best_var))
```
